<!DOCTYPE html>
<html lang="ko-KR" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>DETR 논문(End-to-End Object Detection with Transformers) 리뷰 | VLAD Ops</title>
    <meta name="description" content="VLAD Ops는 머신러닝 통합 솔루션입니다.">
    <meta name="generator" content="VitePress v1.5.0">
    <link rel="preload stylesheet" href="/assets/style.DJbWa6Gi.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    <script type="module" src="/assets/chunks/metadata.61d5b633.js"></script>
    <script type="module" src="/assets/app.Ck5b2tOr.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.COwza3Dz.js">
    <link rel="modulepreload" href="/assets/chunks/framework.CiOUwjDc.js">
    <link rel="modulepreload" href="/assets/blog_posts_2025_detr_paper_review.md.B8giEylB.lean.js">
    <meta property="og:type" content="website">
    <meta property="og:site_name" content="VLAD Ops">
    <meta name="apple-mobile-web-app-status-bar-style" content="default">
    <meta name="keywords" content="VLAD, VLAD Ops, 인공지능 학습, 머신러닝 플랫폼, 머신러닝 솔루션, AI, ML, DL, Machine Learning, Deep Learning">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-P6WWQXT3BW"></script>
    <script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-P6WWQXT3BW");</script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <meta property="og:title" content="VLAD Ops | 머신러닝 통합 솔루션">
    <meta property="og:description" content="VLAD Ops는 머신러닝 통합 솔루션입니다.">
    <meta property="og:url" content="https://linkgenesis-ai.github.io/">
    <meta property="og:locale" content="ko_KR">
    <meta name="apple-mobile-web-app-title" content="VLAD Ops | 머신러닝 통합 솔루션">
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0f60ec36></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0f60ec36> Skip to content </a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/" data-v-1168a8e4><!--[--><!--]--><!----><span data-v-1168a8e4>VLAD Ops</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/guide/intro-vladops" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>제품 가이드</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/reference/overview" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>개발자 문서</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup active" data-v-dc692963 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><!----><span data-v-cf11d7a2>블로그</span><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><div class="items" data-v-b98bc113><!--[--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/blog/" data-v-35975db6><!--[--><span data-v-35975db6>Blog Home</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-b98bc113 data-v-35975db6><a class="VPLink link" href="/blog/archives" data-v-35975db6><!--[--><span data-v-35975db6>Archives</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-6aa21345 data-v-88af2de4 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="언어 변경" data-v-cf11d7a2><span class="text" data-v-cf11d7a2><span class="vpi-languages option-icon" data-v-cf11d7a2></span><!----><span class="vpi-chevron-down text-icon" data-v-cf11d7a2></span></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><div class="items" data-v-88af2de4><p class="title" data-v-88af2de4>한국어</p><!--[--><div class="VPMenuLink" data-v-88af2de4 data-v-35975db6><a class="VPLink link" href="/en/blog/posts/2025/detr_paper_review" data-v-35975db6><!--[--><span data-v-35975db6>English</span><!--]--></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><div class="group translations" data-v-bb2aa2f0><p class="trans-title" data-v-bb2aa2f0>한국어</p><!--[--><div class="VPMenuLink" data-v-bb2aa2f0 data-v-35975db6><a class="VPLink link" href="/en/blog/posts/2025/detr_paper_review" data-v-35975db6><!--[--><span data-v-35975db6>English</span><!--]--></a></div><!--]--></div><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>다크 모드</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav empty fixed" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><!----><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-17a5e62e><button data-v-17a5e62e>맨 위로 돌아가기</button><!----></div></div></div><!----><div class="VPContent" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="left-aside aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--[--><!--[--><!--[--><!--[--><span class="bg-primary-100 inline-flex items-center rounded text-sm font-medium"><div data-v-90d3a8c0><div class="i-[carbon/notebook] mr-2" data-v-90d3a8c0></div><span data-v-90d3a8c0>Article</span></div></span><span class="bg-primary-100 inline-flex rounded text-sm font-medium"><div class="flex flex-wrap gap-2 py-5"><!--[--><a class="rounded-sm bg-gray-100 px-2 py-1 text-xs font-semibold text-gray-600" href="/blog/tags?init=story"><!----> story</a><!--]--></div></span><dl class="pb-10 pt-6 xl:border-b xl:border-gray-200 xl:pt-11 dark:xl:border-slate-200/5" data-v-60983087><dt class="sr-only" data-v-60983087>Authors</dt><dd data-v-60983087><ul class="flex justify-center space-x-8 sm:space-x-12 xl:block xl:space-x-0 xl:space-y-8" data-v-60983087><li class="flex items-center space-x-2" data-v-60983087><img src="https://cdn-icons-png.flaticon.com/64/149/149071.png" alt="author image" class="h-10 w-10 rounded-full" data-v-60983087><dl class="whitespace-nowrap text-sm font-medium leading-5" data-v-60983087><dt class="sr-only" data-v-60983087>Name</dt><dd class="text-gray-900 dark:text-white" data-v-60983087><a href="/blog/authors/sean" class="text-lg text-gray-900 hover:text-[color:var(--vp-c-brand-light)] dark:text-white dark:hover:text-[color:var(--vp-c-brand-dark)]" data-v-60983087>sean</a></dd><!----><!----></dl></li></ul></dd></dl><!--]--><!--]--><!--]--><!--]--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>이 페이지 목차</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--[--><!--[--><!--[--><footer class="mb-24 divide-y divide-gray-200 text-sm font-medium leading-5 dark:divide-slate-200/5" data-v-91c148fa><!----><div class="py-3" data-v-91c148fa><h2 class="text-xs uppercase tracking-wide text-gray-500 dark:text-white" data-v-91c148fa> Previous Article </h2><div class="link" data-v-91c148fa><a href="/blog/posts/2024/post1" data-v-91c148fa>연구실 블로그가 오픈되었습니다.</a></div></div><div class="pt-3" data-v-91c148fa><a class="link" href="/blog/" data-v-91c148fa>← Back to the blog</a></div></footer><!----><!--]--><!--]--><!--]--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--[--><!--[--><!--[--><header class="space-y-1 pt-6 text-center xl:pb-10"><dl><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-300"><time datetime="2025-03-06T00:00:00.000Z">2025/03/06</time></dd></dl><h1 class="md:leading-14 text-3xl font-extrabold leading-9 tracking-tight text-[color:var(--vp-c-brand-dark)] dark:text-[color:var(--vp-c-brand-light)] sm:text-4xl sm:leading-10 md:text-5xl">DETR 논문(End-to-End Object Detection with Transformers) 리뷰</h1></header><!--[--><div class="xs:show xl:hidden flex justify-center space-x-8 sm:space-x-12 xl:block xl:space-x-0 xl:space-y-8"><span class="bg-primary-100 inline-flex items-center rounded text-sm font-medium"><div data-v-90d3a8c0><div class="i-[carbon/notebook] mr-2" data-v-90d3a8c0></div><span data-v-90d3a8c0>Article</span></div></span></div><dl class="pb-10 pt-6 xl:border-b xl:border-gray-200 xl:pt-11 dark:xl:border-slate-200/5 xs:show xl:hidden" data-v-60983087><dt class="sr-only" data-v-60983087>Authors</dt><dd data-v-60983087><ul class="flex justify-center space-x-8 sm:space-x-12 xl:block xl:space-x-0 xl:space-y-8" data-v-60983087><li class="flex items-center space-x-2" data-v-60983087><img src="https://cdn-icons-png.flaticon.com/64/149/149071.png" alt="author image" class="h-10 w-10 rounded-full" data-v-60983087><dl class="whitespace-nowrap text-sm font-medium leading-5" data-v-60983087><dt class="sr-only" data-v-60983087>Name</dt><dd class="text-gray-900 dark:text-white" data-v-60983087><a href="/blog/authors/sean" class="text-lg text-gray-900 hover:text-[color:var(--vp-c-brand-light)] dark:text-white dark:hover:text-[color:var(--vp-c-brand-dark)]" data-v-60983087>sean</a></dd><!----><!----></dl></li></ul></dd></dl><!--]--><!--]--><!----><!--]--><!--]--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _blog_posts_2025_detr_paper_review" data-v-39a288b8><div><p>ECCV 2020에서 발표된 DETR(End-to-End Object Detection with Transformers) 논문 리뷰입니다.</p><p>DETR은 기존 NLP 테스크에서 주로 사용되던 Transformer 구조를 Object Detection에 적용한 모델로, end-to-end 방식으로 객체 탐지를 수행하면서도 높은 성능을 보이는 것이 특징입니다. 또한 현재 Object Detection 분야의 SOTA 모델 중 상당수가 DETR을 기반으로 발전했기 때문에, 한 번쯤 읽어볼 가치가 있는 논문입니다.</p><p>이 리뷰에서는 NLP 테스크에서의 Transformer 구조와 Object Detection 테스크에서의 차이점, DETR 모델의 Loss 함수 분석, 학습 비용 및 방법에 대해 다룰 예정입니다. <br><br></p><h2 id="문제-정의" tabindex="-1">문제 정의 <a class="header-anchor" href="#문제-정의" aria-label="Permalink to &quot;문제 정의&quot;">​</a></h2><p>기존 object detection 모델에서는 다수의 anchor 생성, NMS와 같은 후처리 과정이 무조건적으로 진행되고 있습니다.</p><p>이 논문에서 저자는 이와 같은 후 처리 과정을 사용할 필요없이, 이진 매칭을 통해 중복 예측을 방지하는 transformer 기반의 end to end 모델 DETR을 제시합니다. <br><br></p><h2 id="transformer-for-nlp-task-vs-detr-transformer" tabindex="-1">Transformer for NLP task vs DETR Transformer <a class="header-anchor" href="#transformer-for-nlp-task-vs-detr-transformer" aria-label="Permalink to &quot;Transformer for NLP task vs DETR Transformer&quot;">​</a></h2><p><img src="/blog-data/comparison_transformer_architecture.png" alt="트랜스포머 구조 비교"></p><p>NLP 테스크에서 사용되는 트랜스포머 구조와 DETR(Detection Transformer)의 가장 큰 차이점은 Self-Attention과 Object Query의 유무입니다. 기존 NLP 트랜스포머에서는 포함되어 있지 않은 Self-Attention 방식과 Object Query 개념이 DETR 구조에서 중요한 요소로 자리 잡고 있습니다. 이 두 가지 개념에 대해 자세히 살펴보겠습니다. <br><br></p><h3 id="self-attention" tabindex="-1">Self Attention <a class="header-anchor" href="#self-attention" aria-label="Permalink to &quot;Self Attention&quot;">​</a></h3><p>Object Detection 태스크에서 Self-Attention은 속도 향상에 중요한 요소로 작용합니다. 일반적인 Attention을 사용하면 여러 레이어를 거치면서 각각 Query, Key, Value 값을 연산해야 하지만, Self-Attention은 한 레이어에서 한 번의 입출력만으로 Query, Key, Value를 생성할 수 있습니다. 이러한 특성 덕분에 DETR에서 Self-Attention이 적용되었다고 볼 수 있습니다.</p><p>또한, DETR의 Decoder 구조를 살펴보면 마지막에는 Self-Attention 대신 일반 Attention Layer가 사용됩니다. 이는 이미 입력 값인 Query, Key, Value가 생성된 상태에서 추가적인 변형 없이 그대로 사용하는 것이 더 좋은 성능을 내기 때문입니다. 결과적으로 DETR의 구조는 이러한 점들을 고려하여 효율적으로 최적화되었다고 볼 수 있습니다. <br><br></p><h3 id="object-query" tabindex="-1">Object Query <a class="header-anchor" href="#object-query" aria-label="Permalink to &quot;Object Query&quot;">​</a></h3><p>Object Query는 Object Query Features, Object Query Positional Embedding (위치 정보 제공)로 구성되어 있으며, 이 두 가지는 모두 학습이 가능한 요소입니다.</p><ol><li>Object Query Features (주체, 주요 역할) <ul><li>디코더의 초기 입력 값으로 사용되며, 초기 값은 0으로 설정됩니다.</li><li>고정된 개수(N개)만큼 생성되며, 학습 전에 초기화됩니다.</li><li>이 개수는 하이퍼파라미터로 설정할 수 있으며, 논문에서는 100개로 설정되었습니다.</li><li>객체의 bounding box (bbox) 정보를 포함하고 있습니다.</li><li>디코더의 각 레이어를 통과하면서 계속 업데이트됩니다. <ul><li>이때, spatial positional encoding과 positional embedding의 위치 정보가 업데이트를 돕습니다.</li></ul></li><li>즉, 매 디코더 레이어마다 새로운 정보가 반영되면서 점점 더 정확한 객체 정보를 갖게 됩니다.</li></ul></li><li>Object Query Positional Embedding (위치 정보 제공) <ul><li>학습 전에 초기화되며, 객체 쿼리의 위치 정보를 인코딩하는 역할을 합니다.</li><li>모델이 각 쿼리가 어떤 객체를 예측하는지 학습할 수 있도록 돕습니다.</li><li>순전파(Forward Pass)에서는 업데이트되지 않고,</li><li>역전파(Backpropagation) 이후에만 업데이트됩니다.</li></ul></li></ol><h3 id="요약" tabindex="-1">요약: <a class="header-anchor" href="#요약" aria-label="Permalink to &quot;요약:&quot;">​</a></h3><ul><li>Object Query Features는 객체를 탐색하는 주요 정보이며, 디코더를 거치면서 점점 업데이트됩니다.</li><li>Object Query Positional Embedding은 각 쿼리의 위치 정보를 제공하는 역할을 하며, 역전파 이후에만 업데이트됩니다. <br><br></li></ul><h2 id="set-prediction-loss" tabindex="-1">Set Prediction Loss <a class="header-anchor" href="#set-prediction-loss" aria-label="Permalink to &quot;Set Prediction Loss&quot;">​</a></h2><p><img src="/blog-data/set_prediction_loss1.png" alt="loss 함수 비교1"></p><p>𝑁 == object query의 지정된 예측 개수(논문에서는 100개로 set)<br> 𝑐𝑖 == ground truth class<br> 𝑏𝑖 == ground truth bbox</p><p>빨간색으로 표시된 부분은 매칭 함수이며, 최적의 매칭 값을 보이는 bbox의 인덱스 값을 산출합니다. <br><br></p><p><img src="/blog-data/set_prediction_loss2.png" alt="loss 함수 비교2"></p><p>DETR 모델은 box loss 계산을 위하여 GIoU와 L1 loss의 조합을 사용합니다.</p><h3 id="giou-l1-loss" tabindex="-1">GIoU + L1 loss: <a class="header-anchor" href="#giou-l1-loss" aria-label="Permalink to &quot;GIoU + L1 loss:&quot;">​</a></h3><p>가장 일반적으로 사용되는 L1 loss는, 작은 박스와 큰박스의 상대오차가 비슷하더라도 서로 다른 크기의 값을 가지게 됩니다. 이러한 문제를 완화하기 위해 DETR 모델은 L1 loss와 GIoU(generalized IoU loss)의 조합을 사용합니다.</p><h3 id="giou란" tabindex="-1">GIoU란, <a class="header-anchor" href="#giou란" aria-label="Permalink to &quot;GIoU란,&quot;">​</a></h3><ul><li>기존 IoU - (C box 중 A와 B 모두와 겹치지 않는 영역의 비율)</li><li>ground truth bbox에 대해서 overlap하기 위해 영역을 넓혔다가, 다시 IoU 값을 높이기 위해 bbox의 영역을 축소시키는 과정을 통해 값이 산출됩니다.</li><li>겹치지 않는 박스에 대한 기울기 소실(gradient vanishing) 문제는 개선했지만, 수렴 속도가 느리고 부정확하게 (horizontal과 vertical 정보 표현이 X) 박스를 예측한다는 문제점이 존재합니다. <br><br></li></ul><h2 id="사용된-데이터-셋" tabindex="-1">사용된 데이터 셋 <a class="header-anchor" href="#사용된-데이터-셋" aria-label="Permalink to &quot;사용된 데이터 셋&quot;">​</a></h2><ul><li>coco 2017 object detection dataset <br><br></li></ul><h2 id="학습-및-추론-비용" tabindex="-1">학습 및 추론 비용 <a class="header-anchor" href="#학습-및-추론-비용" aria-label="Permalink to &quot;학습 및 추론 비용&quot;">​</a></h2><p>논문에서 언급된 바로는, 베이스 모델 학습시 300 epoch로 16개의 V100 GPU로 3일간 진행이 되었다고 합니다. (batch size 64, gpu당 4장씩 학습)</p><p>아래는 논문에서 발췌한 모델별 추론 성능표입니다. 논문에서 비교가 된 Faster RCNN의 경우도 함께 적혀있으니 참고하시면 좋을 것 같습니다.</p><p><img src="/blog-data/model_train_cost.png" alt="추론 비용"></p><h3 id="메인-코드-및-모델-다운로드" tabindex="-1">메인 코드 및 모델 다운로드: <a class="header-anchor" href="#메인-코드-및-모델-다운로드" aria-label="Permalink to &quot;메인 코드 및 모델 다운로드:&quot;">​</a></h3><ul><li><a href="https://github.com/facebookresearch/detr" target="_blank" rel="noreferrer">https://github.com/facebookresearch/detr</a></li></ul></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--[--><!--[--><!--[--><footer class="mb-24 divide-y divide-gray-200 text-sm font-medium leading-5 dark:divide-slate-200/5 xs:show lg:hidden" data-v-91c148fa><!----><div class="py-3" data-v-91c148fa><h2 class="text-xs uppercase tracking-wide text-gray-500 dark:text-white" data-v-91c148fa> Previous Article </h2><div class="link" data-v-91c148fa><a href="/blog/posts/2024/post1" data-v-91c148fa>연구실 블로그가 오픈되었습니다.</a></div></div><div class="pt-3" data-v-91c148fa><a class="link" href="/blog/" data-v-91c148fa>← Back to the blog</a></div></footer><!--]--><!--]--><!--]--><!--]--><div class="edit-info" data-v-e257564d><!----><div class="last-updated" data-v-e257564d><p class="VPLastUpdated" data-v-e257564d data-v-e98dd255>업데이트 날짜: <time datetime="2025-04-23T01:22:55.000Z" data-v-e98dd255></time></p></div></div><!----></footer><!--[--><!--[--><!--[--><!----><!--]--><!--]--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    
    
  </body>
</html>